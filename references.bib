@article{kirkpatrick2017ewc,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  journal={Proceedings of the National Academy of Sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017}
}

@article{luo2023forgetting,
  title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023}
}

@article{li2024revisiting,
  title={Revisiting Catastrophic Forgetting in Large Language Model Tuning},
  author={Li, Hongyu and Ding, Liang and Fang, Meng and Tao, Dacheng},
  journal={Findings of EMNLP},
  year={2024}
}

@inproceedings{hu2021lora,
  title={{LoRA}: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{yuan2023asvd,
  title={{ASVD}: Activation-aware Singular Value Decomposition for Compressing Large Language Models},
  author={Yuan, Zhihang and Shang, Yuzhang and Song, Yue and Yang, Dawei and Wu, Qiang and Yan, Yan and Sun, Guangyu},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}

@inproceedings{wang2024svdllm,
  title={{SVD-LLM}: Truncation-aware Singular Value Decomposition for Large Language Model Compression},
  author={Wang, Xin and Zheng, Yu and Wan, Zhongwei and Zhang, Mi},
  booktitle={International Conference on Learning Representations},
  year={2025}
}

@inproceedings{hsu2022fwsvd,
  title={Language model compression with weighted low-rank factorization},
  author={Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{kaushal2023lord,
  title={{LoRD}: Low Rank Decomposition Of Monolingual Code {LLMs} For One-Shot Compression},
  author={Kaushal, Ayush and Vaidhya, Tejas and Rish, Irina},
  journal={arXiv preprint arXiv:2309.14021},
  year={2023}
}

@inproceedings{ashkboos2024slicegpt,
  title={{SliceGPT}: Compress Large Language Models by Deleting Rows and Columns},
  author={Ashkboos, Saleh and Croci, Maximilian L and Gennari do Nascimento, Marcelo and Hoefler, Torsten and Hensman, James},
  booktitle={International Conference on Learning Representations},
  year={2024}
}

@inproceedings{frantar2022gptq,
  title={{GPTQ}: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{lin2023awq,
  title={{AWQ}: Activation-aware Weight Quantization for {LLM} Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
  booktitle={MLSys},
  year={2024}
}

@inproceedings{kim2023squeezellm,
  title={{SqueezeLLM}: Dense-and-Sparse Quantization},
  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@inproceedings{dettmers2023qlora,
  title={{QLoRA}: Efficient Finetuning of Quantized {LLMs}},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{guo2023lqlora,
  title={{LQ-LoRA}: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning},
  author={Guo, Han and Greengard, Philip and Xing, Eric P and Kim, Yoon},
  journal={arXiv preprint arXiv:2311.12023},
  year={2023}
}

@inproceedings{saha2024caldera,
  title={Compressing Large Language Models using Low Rank and Low Precision Decomposition},
  author={Saha, Rajarshi and Sagan, Naomi and Srivastava, Varun and Goldsmith, Andrea J and Pilanci, Mert},
  booktitle={Advances in Neural Information Processing Systems},
  year={2024}
}

@article{lee2019freezing,
  title={What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning},
  author={Lee, Jaejun and Tang, Raphael and Lin, Jimmy},
  journal={arXiv preprint arXiv:1911.03090},
  year={2019}
}

@inproceedings{houlsby2019adapters,
  title={Parameter-Efficient Transfer Learning for {NLP}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@inproceedings{zhang2023adalora,
  title={{AdaLoRA}: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning},
  author={Zhang, Qingru and Chen, Minshuo and Bukharin, Alexander and Karampatziakis, Nikos and He, Pengcheng and Cheng, Yu and Chen, Weizhu and Zhao, Tuo},
  booktitle={International Conference on Learning Representations},
  year={2023}
}

@inproceedings{liu2024dora,
  title={{DoRA}: Weight-Decomposed Low-Rank Adaptation},
  author={Liu, Shih-Yang and Wang, Chien-Yi and Yin, Hongxu and Molchanov, Pavlo and Wang, Yu-Chiang Frank and Cheng, Kwang-Ting and Chen, Min-Hung},
  booktitle={International Conference on Machine Learning},
  year={2024}
}
